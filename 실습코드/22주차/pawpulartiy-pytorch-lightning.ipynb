{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is Pytorch Lightning?\n\nPyTorch Lightning은 PyTorch에 대한 High-level 인터페이스를 제공하는 오픈소스 Python 라이브러리입니다. PyTorch만으로도 충분히 다양한 AI 모델들을 쉽게 생성할 수 있지만 GPU나 TPU, 그리고 16-bit precision, 분산학습 등 더욱 복잡한 조건에서 실험하게 될 경우, 코드가 복잡해집니다. 따라서 코드의 추상화를 통해, 프레임워크를 넘어 하나의 코드 스타일로 자리 잡기 위해 탄생한 프로젝트가 바로 PyTorch Lightning입니다.\n\nhttps://baeseongsu.github.io/posts/pytorch-lightning-introduction/","metadata":{}},{"cell_type":"markdown","source":"# Description\nLet's predict **engagement** with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.\n\n## How Pawpularity Score Is Derived\nThe Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\nDuplicate clicks, crawler bot accesses and sponsored profiles are excluded from the analysis.\n\n## Purpose of Photo Metadata\nWe have included optional Photo Metadata, manually labeling each photo for key visual quality and composition parameters.\nThese labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\nYou may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\nIn our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.\nTraining Data\ntrain/ - Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\ntrain.csv - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The Id column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n\n## Example Test Data\nIn addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission).\n\ntest/ - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\ntest.csv - Randomly generated metadata similar to the training set metadata.\nsample_submission.csv - A sample submission file in the correct format.\n\n## Photo Metadata\nThe train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n\n* Focus - Pet stands out against uncluttered background, not too close / far.\n* Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n* Face - Decently clear face, facing front or near-front.\n* Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n* Action - Pet in the middle of an action (e.g., jumping).\n* Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n* Group - More than 1 pet in the photo.\n* Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n* Human - Human in the photo.\n* Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n* Info - Custom-added text or labels (i.e. pet name, description).\n* Blur - Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0.","metadata":{}},{"cell_type":"code","source":"!pip install python-box timm pytorch-lightning==1.4.0 grad-cam ttach","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:37:31.275780Z","iopub.execute_input":"2022-08-11T07:37:31.276218Z","iopub.status.idle":"2022-08-11T07:37:55.390783Z","shell.execute_reply.started":"2022-08-11T07:37:31.276148Z","shell.execute_reply":"2022-08-11T07:37:55.389868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport warnings\nfrom pprint import pprint\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom box import Box\nfrom timm import create_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule\n\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:37:55.393098Z","iopub.execute_input":"2022-08-11T07:37:55.393407Z","iopub.status.idle":"2022-08-11T07:38:02.784125Z","shell.execute_reply.started":"2022-08-11T07:37:55.393364Z","shell.execute_reply":"2022-08-11T07:38:02.783229Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"config = {'seed': 2021,\n          'root': '/kaggle/input/petfinder-pawpularity-score/', \n          'n_splits': 5,\n          'epoch': 20,\n          'trainer': {\n              'gpus': 1,\n              'accumulate_grad_batches': 1,\n              'progress_bar_refresh_rate': 1,\n              'fast_dev_run': False,\n              'num_sanity_val_steps': 0,\n              'resume_from_checkpoint': None,\n          },\n          'transform':{\n              'name': 'get_default_transforms',\n              'image_size': 224\n          },\n          'train_loader':{\n              'batch_size': 64,\n              'shuffle': True,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': True,\n          },\n          'val_loader': {\n              'batch_size': 64,\n              'shuffle': False,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': False\n         },\n          'model':{\n              'name': 'swin_tiny_patch4_window7_224',\n              'output_dim': 1\n          },\n          'optimizer':{\n              'name': 'optim.AdamW',\n              'params':{\n                  'lr': 1e-5\n              },\n          },\n          'scheduler':{\n              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n              'params':{\n                  'T_0': 20,\n                  'eta_min': 1e-4,\n              }\n          },\n          'loss': 'nn.BCEWithLogitsLoss',\n}\n\nconfig = Box(config)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-11T07:38:02.785540Z","iopub.execute_input":"2022-08-11T07:38:02.785782Z","iopub.status.idle":"2022-08-11T07:38:02.793757Z","shell.execute_reply.started":"2022-08-11T07:38:02.785741Z","shell.execute_reply":"2022-08-11T07:38:02.792316Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## dataset","metadata":{}},{"cell_type":"code","source":"class PetfinderDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image\n\nclass PetfinderDataModule(LightningDataModule):\n    def __init__(\n        self,\n        train_df,\n        val_df,\n        cfg,\n    ):\n        super().__init__()\n        self._train_df = train_df\n        self._val_df = val_df\n        self._cfg = cfg\n\n    def __create_dataset(self, train=True):\n        return (\n            PetfinderDataset(self._train_df, self._cfg.transform.image_size)\n            if train\n            else PetfinderDataset(self._val_df, self._cfg.transform.image_size)\n        )\n\n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset, **self._cfg.train_loader)\n\n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset, **self._cfg.val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:02.824777Z","iopub.execute_input":"2022-08-11T07:38:02.825065Z","iopub.status.idle":"2022-08-11T07:38:02.837685Z","shell.execute_reply.started":"2022-08-11T07:38:02.825041Z","shell.execute_reply":"2022-08-11T07:38:02.836961Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## visualize data","metadata":{}},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)\nseed_everything(config.seed)\n\ndf = pd.read_csv(os.path.join(config.root, \"train.csv\"))\ndf[\"Id\"] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:02.838773Z","iopub.execute_input":"2022-08-11T07:38:02.839076Z","iopub.status.idle":"2022-08-11T07:38:02.922218Z","shell.execute_reply.started":"2022-08-11T07:38:02.839015Z","shell.execute_reply":"2022-08-11T07:38:02.921581Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sample_dataloader = PetfinderDataModule(df, df, config).val_dataloader()\nimages, labels = iter(sample_dataloader).next()\n\nplt.figure(figsize=(12, 12))\nfor it, (image, label) in enumerate(zip(images[:16], labels[:16])):\n    plt.subplot(4, 4, it+1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title(f'Pawpularity: {int(label)}')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:02.923176Z","iopub.execute_input":"2022-08-11T07:38:02.923466Z","iopub.status.idle":"2022-08-11T07:38:06.777844Z","shell.execute_reply.started":"2022-08-11T07:38:02.923431Z","shell.execute_reply":"2022-08-11T07:38:06.776950Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## augmentation","metadata":{}},{"cell_type":"code","source":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:06.779160Z","iopub.execute_input":"2022-08-11T07:38:06.779395Z","iopub.status.idle":"2022-08-11T07:38:06.788172Z","shell.execute_reply.started":"2022-08-11T07:38:06.779363Z","shell.execute_reply":"2022-08-11T07:38:06.787416Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"markdown","source":"### assert 함수?\n어떤 함수는 성능을 높이기 위해 반드시 정수만을 입력받아 처리하도록 만들 수 있다. 이런 함수를 만들기 위해서는 반드시 함수에 정수만 들어오는지 확인할 필요가 있다. 이를 위해 if문을 사용할 수도 있고 '예외 처리'를 사용할 수도 있지만 '가정 설정문'을 사용하는 방법도 있다.\n\nassert는 개발자가 프로그램을 만드는 과정에 관여한다. 원하는 조건의 변수 값을 보증받을 때까지 assert로 테스트 할 수 있다.\n이는 단순히 에러를 찾는 것이 아니라 값을 보증하기 위해 사용된다.\n\n예를 들어 함수의 입력 값이 어떤 조건의 참임을 보증하기 위해 사용할 수 있고 함수의 반환 값이 어떤 조건에 만족하도록 만들 수 있다. 혹은 변수 값이 변하는 과정에서 특정 부분은 반드시 어떤 영역에 속하는 것을 보증하기 위해 가정 설정문을 통해 확인 할 수도 있다.\n\n이처럼 실수를 가정해 값을 보증하는 방식으로 코딩 하기 때문에 이를 '방어적 프로그래밍'이라 부른다.\nhttps://wikidocs.net/21050","metadata":{}},{"cell_type":"code","source":"def mixup(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0])\n    mixed_x = lam * x + (1 - lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n\nclass Model(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=True, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'train')\n        return {'loss': loss, 'pred': pred, 'labels': labels}\n        \n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'val')\n        return {'pred': pred, 'labels': labels}\n    \n    def __share_step(self, batch, mode):\n        images, labels = batch\n        labels = labels.float() / 100.0\n        images = self.transform[mode](images)\n        \n        if torch.rand(1)[0] < 0.5 and mode == 'train':\n            mix_images, target_a, target_b, lam = mixup(images, labels, alpha=0.5)\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + \\\n                (1 - lam) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n        \n        pred = logits.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        return loss, pred, labels\n        \n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'train')\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'val')    \n        \n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out['pred'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        self.log(f'{mode}_loss', metrics)\n    \n    def check_gradcam(self, dataloader, target_layer, target_category, reshape_transform=None):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer, \n            use_cuda=self.cfg.trainer.gpus, \n            reshape_transform=reshape_transform)\n        \n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform['val'](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n        \n        grayscale_cam = cam(input_tensor=images, target_category=target_category, eigen_smooth=True)\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255.\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer,\n            **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:06.789565Z","iopub.execute_input":"2022-08-11T07:38:06.790107Z","iopub.status.idle":"2022-08-11T07:38:06.818412Z","shell.execute_reply.started":"2022-08-11T07:38:06.790073Z","shell.execute_reply":"2022-08-11T07:38:06.817703Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    datamodule = PetfinderDataModule(train_df, val_df, config)\n    model = Model(config)\n    earystopping = EarlyStopping(monitor=\"val_loss\")\n    lr_monitor = callbacks.LearningRateMonitor()\n    loss_checkpoint = callbacks.ModelCheckpoint(\n        filename=\"best_loss\",\n        monitor=\"val_loss\",\n        save_top_k=1,\n        mode=\"min\",\n        save_last=False,\n    )\n    logger = TensorBoardLogger(config.model.name)\n    \n    trainer = pl.Trainer(\n        logger=logger,\n        max_epochs=config.epoch,\n        callbacks=[lr_monitor, loss_checkpoint, earystopping],\n        **config.trainer,\n    )\n    trainer.fit(model, datamodule=datamodule)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T07:38:06.820831Z","iopub.execute_input":"2022-08-11T07:38:06.821183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# class activation map","metadata":{}},{"cell_type":"code","source":"# gradcam reshape_transform for vit\ndef reshape_transform(tensor, height=7, width=7):\n    result = tensor.reshape(tensor.size(0),\n                            height, width, tensor.size(2))\n\n    # like in CNNs.\n    result = result.permute(0, 3, 1, 2)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(config) \nmodel.load_state_dict(torch.load(f'{config.model.name}/default/version_0/checkpoints/best_loss.ckpt')['state_dict'])\nmodel = model.cuda().eval()\nconfig.val_loader.batch_size = 16\ndatamodule = PetfinderDataModule(train_df, val_df, config)\nimages, grayscale_cams, preds, labels = model.check_gradcam(\n                                            datamodule.val_dataloader(), \n                                            target_layer=model.backbone.layers[-1].blocks[-1].norm1,\n                                            target_category=None,\n                                            reshape_transform=reshape_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor it, (image, grayscale_cam, pred, label) in enumerate(zip(images, grayscale_cams, preds, labels)):\n    plt.subplot(4, 4, it + 1)\n    visualization = show_cam_on_image(image, grayscale_cam)\n    plt.imshow(visualization)\n    plt.title(f'pred: {pred:.1f} label: {label}')\n    plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize result","metadata":{}},{"cell_type":"code","source":"from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\npath = glob(f'./{config.model.name}/default/version_0/events*')[0]\nevent_acc = EventAccumulator(path, size_guidance={'scalars': 0})\nevent_acc.Reload()\n\nscalars = {}\nfor tag in event_acc.Tags()['scalars']:\n    events = event_acc.Scalars(tag)\n    scalars[tag] = [event.value for event in events]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set()\n\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(len(scalars['lr-AdamW'])), scalars['lr-AdamW'])\nplt.xlabel('epoch')\nplt.ylabel('lr')\nplt.title('adamw lr')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(scalars['train_loss'])), scalars['train_loss'], label='train_loss')\nplt.plot(range(len(scalars['val_loss'])), scalars['val_loss'], label='val_loss')\nplt.legend()\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.title('train/val rmse')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('best_val_loss', min(scalars['val_loss']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}